{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3335529-4650-4890-9004-803b12305e6e",
   "metadata": {},
   "source": [
    "# 1 Langchain概述与环境搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d1651c-f978-4375-9394-27aaba81bcb5",
   "metadata": {},
   "source": [
    "## 1.1 Langchain简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917cc29-b3b3-4475-bd7b-728fece29075",
   "metadata": {},
   "source": [
    "在大模型技术飞速发展的浪潮中，单一的大模型往往难以满足复杂场景下的应用需求 —— 例如，无法直接处理本地文档、难以串联多步任务、缺乏与外部工具的协同能力。Langchain 的出现，正是为了解决这些痛点，它像一座 “桥梁”，将大模型与各类数据、工具、服务连接起来，让开发者能够更高效地构建功能强大、场景适配的 AI 应用。本节将从背景发展、核心功能、应用场景三个维度，带你全面认识 Langchain。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074befa-44a2-40ae-b822-550217656d35",
   "metadata": {},
   "source": [
    "### 1.1.1 Langchain的背景与发展"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa2241-9326-446d-a2ee-b6ebb5049984",
   "metadata": {},
   "source": [
    "### 1.1.2 Langchain的核心功能与特点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fc0fc4-b794-4159-a460-9d8c17d4bfa6",
   "metadata": {},
   "source": [
    "Langchain的价值在于其“模块化”与“可拓展性”，让开发者能够像搭积木一样构建AI应用。其核心功能与特点可概括为以下四个点：\n",
    "\n",
    "1. 核心功能\n",
    "\n",
    "    - **文档加载与处理**：支持多种格式文档的加载，并支持文本分割、Embedding生成、向量存储等处理能力。\n",
    "\n",
    "    - **Chain的构建与管理**：Chain是Langchain的核心概念，指将大模型的单一任务串联起来，形成任务流的过程。\n",
    "\n",
    "    - **Agents的支持**：智能体是具备自主决策能力的组件，可根据用户需求，自主判断需要调用哪些工具、按什么顺序调用、是否需要用户补充信息等。\n",
    "\n",
    "    - **与外部工具/服务的集成**：Langchain已预置与外部工具的集成接口，如大模型、数据库、工具服务等\n",
    "\n",
    "2. 核心特点\n",
    "\n",
    "- **模块化**：所有功能均以“组件”形式存在，开发AI应用更像是在做排列组合。\n",
    "\n",
    "- **可拓展性**：除了Langchain中自带的组件，也可以自定义组件，以满足个性化需求\n",
    "\n",
    "- **兼容性强**：与主流编程语言（Python、JS/TS）、开发框架（FastAPI、Streamlit）、部署平台（Docker、云服务器）兼容，降低开发与部署的门槛。\n",
    "\n",
    "- **聚焦实用场景**：所有功能设计均围绕“解决实际痛点”展开\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c581c6-0a36-43d4-9a04-c759d0571ab1",
   "metadata": {},
   "source": [
    "### 1.1.3 Langchain的应用场景"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6759540d-e54f-4176-aa08-629e123f6af9",
   "metadata": {},
   "source": [
    "1. 智能问答与知识库系统\n",
    "\n",
    "**场景需求**：企业需要一个 “能理解内部文档（如产品手册、员工手册、技术文档）的问答系统”，让员工 / 客户快速获取精准信息，替代传统 “人工查文档 + 回复” 的低效模式。\n",
    "\n",
    "**Langchain 解决方案**：通过 “文档加载器” 加载内部文档→“文本分割 + Embedding” 处理文档→存储到向量数据库→构建 “检索增强生成（RAG）链”→用户提问时，链自动查询向量数据库获取相关文档片段，结合大模型生成回答。\n",
    "\n",
    "**案例**：某科技公司用 Langchain 构建 “产品售后知识库”，客户提问 “如何解决设备连接失败问题” 时，系统自动查询产品手册中相关章节，生成步骤清晰的解决方案，售后响应时间从 30 分钟缩短至 1 分钟。\n",
    "\n",
    "2. 文档摘要与信息提取\n",
    "\n",
    "**场景需求**：职场人需要快速从大量文档（如会议纪要、行业报告、合同文件）中提取关键信息（如核心结论、待办事项、风险条款），避免逐字阅读的耗时问题。\n",
    "\n",
    "**Langchain 解决方案**：通过 “文档加载器” 加载多份文档→“文本分割器” 拆分长文档→构建 “摘要链” 或 “信息提取链”→大模型按预设格式（如 “核心结论：XXX；待办事项：XXX”）输出结果。\n",
    "\n",
    "**案例**：某咨询公司用 Langchain 处理行业报告，原本需要 2 小时阅读的 30 页报告，系统可在 5 分钟内生成 “核心观点 + 数据亮点 + 趋势预测” 的摘要，同时提取报告中的关键数据表格，提升分析效率。\n",
    "\n",
    "3. 自动化任务与工作流\n",
    "\n",
    "**场景需求**：企业需要自动化处理重复性工作（如 “每日汇总销售数据→生成日报→发送到指定邮箱”“根据客户咨询内容，自动创建工单并分配给对应部门”），减少人工操作成本。\n",
    "\n",
    "**Langchain 解决方案**：构建 “智能体 + 工具链”，智能体接收任务需求后，自主调用 “数据库查询工具”“Excel 生成工具”“邮件发送工具”“工单系统接口”，按流程完成任务。\n",
    "\n",
    "**案例**：某电商公司用 Langchain 构建 “销售日报自动化系统”，每天凌晨自动查询 MySQL 数据库中的销售数据，生成包含 “销售额、订单量、Top5 商品” 的 Excel 日报，再通过企业邮箱发送给销售团队，原本 1 小时的人工工作实现全自动化。\n",
    "\n",
    "4. 多模态内容生成\n",
    "\n",
    "**场景需求**：内容创作者需要生成 “文本 + 图像 + 音频” 的多模态内容（如 “根据产品描述生成宣传文案 + 产品示意图 + 语音介绍”“根据课程大纲生成课件文档 + 教学视频脚本”），提升内容丰富度。\n",
    "\n",
    "**Langchain 解决方案**：通过 “文本生成链” 生成文案 / 脚本→调用 “图像生成工具”（如 DALL・E、MidJourney API）生成图像→调用 “音频生成工具”（如 ElevenLabs API）生成语音→串联所有内容生成最终成果。\n",
    "\n",
    "**案例**：某教育机构用 Langchain 开发 “课件生成助手”，老师输入课程主题（如 “初中数学勾股定理”），系统先生成课件文档（包含知识点、例题），再生成教学 PPT 的示意图，最后生成知识点讲解的语音，课件制作时间从 8 小时缩短至 1 小时。\n",
    "\n",
    "5. 代码生成与开发辅助\n",
    "\n",
    "**场景需求**：开发者需要辅助工具完成 “根据需求文档生成代码”“排查代码错误”“生成 API 文档” 等工作，提升开发效率。\n",
    "\n",
    "**Langchain 解决方案**：构建 “代码生成链”，将需求文档（如 “用 Python 写一个读取 Excel 文件并计算平均值的函数”）传给大模型，结合 Langchain 的 “代码执行器” 验证代码正确性，生成可直接使用的代码；或构建 “代码排查链”，输入错误日志，大模型分析错误原因并给出修改建议。\n",
    "\n",
    "**案例**：某软件公司用 Langchain 辅助后端开发，开发者输入 “设计一个用户登录接口，包含手机号验证码验证、密码加密存储功能”，系统生成符合公司代码规范的 Python（FastAPI）代码，同时生成接口文档，开发者仅需微调即可上线，接口开发时间从 4 小时缩短至 30 分钟。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb948291-45b2-4beb-8b7c-684412fc07b5",
   "metadata": {},
   "source": [
    "## 1.2 环境搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adad2ac5-9752-4b2b-8888-306c97a71907",
   "metadata": {},
   "source": [
    "### 1.2.1 安装Langchain及依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6896495f-6457-4bad-8c04-16920bb9ff2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.11\n"
     ]
    }
   ],
   "source": [
    "# 建议Python版本在3.8以上\n",
    "\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abada46-55a6-465b-8744-ebaccdce3b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装Langchain核心包\n",
    "\n",
    "#!pip install langchain\n",
    "\n",
    "# 或升级最新版本\n",
    "!pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e5579bf-f569-4ff9-b605-4f1007e31809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.27\n"
     ]
    }
   ],
   "source": [
    "# 验证是否安装成功\n",
    "\n",
    "!python -c \"import langchain;print(langchain.__version__)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c815166d-28b7-49bf-92b4-86bda0a6a363",
   "metadata": {},
   "source": [
    "**安装场景化依赖包**\n",
    "\n",
    "Langchain很多功能都需要依赖第三方库（如调用OpenAI大模型需要`openai`包），以下是“高频依赖包清单”：\n",
    "\n",
    "|依赖包名称|用途|安装命令|\n",
    "|--|--|--|\n",
    "|`openai`|调用OpenAI大模型(如GPT-3.5、GPT-4)|`pip install openai`|\n",
    "|`python-dotenv`|加载`.env`文件中的环境变量（如API密钥）|`pip install python-dotenv`|\n",
    "|`PyPDF`|加载与处理PDF文档|`pip install pypdf`|\n",
    "|`chromadb`|轻量级向量数据库（存储文档Embedding）|`pip install chromadb`|\n",
    "|`tiktoken`|OpenAI官方分词器（计算token数量）|`pip install tiktoken`|\n",
    "|`requests`|发送HTTP请求（调用第三方API）|`pip install requests`|\n",
    "|`pandas`|处理表格数据（如Excel、CSV文件）|`pip install pandas`|\n",
    "\n",
    "如果是在anaconda环境下可以使用以下命令安装第三方依赖包：\n",
    "\n",
    "```\n",
    "conda install -c conda-forge openai\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa98c10-2ea5-4c11-b719-16e09fc0fcbd",
   "metadata": {},
   "source": [
    "### 1.2.2 配置与初始化Langchain环境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6161c399-6151-4003-97d6-e3b500d2a47b",
   "metadata": {},
   "source": [
    "Langchain 的环境配置核心是 “管理 API 密钥” 和 “初始化核心组件”，其中 API 密钥（如 OpenAI API 密钥）是调用大模型的 “通行证”，需重点注意安全存储（避免明文写在代码中）。以下是标准化配置流程：\n",
    "\n",
    "**1. 管理 API 密钥：使用.env 文件（推荐）**\n",
    "\n",
    "为避免 API 密钥泄露（如代码上传到 GitHub 时被公开），推荐用python-dotenv库加载本地.env文件中的密钥，步骤如下：\n",
    "\n",
    "- **创建.env文件**：在你的项目文件夹中，新建一个名为`.env`的文件，用记事本打开，输入："
   ]
  },
  {
   "cell_type": "raw",
   "id": "88038669-c186-45b5-8da7-309822a30a95",
   "metadata": {},
   "source": [
    "OPENAI_API_KEY=\"sk-你的OpenAI API密钥\"  # 核心密钥，调用OpenAI大模型需用\n",
    "LANGCHAIN_API_KEY=\"lc-你的Langchain API密钥\"  # 可选，使用Langchain云服务时需用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aea3f3-4632-4e24-915f-aaeb85c0963f",
   "metadata": {},
   "source": [
    "- **在代码中加载.env文件**：编写 Python 代码时，通过以下diamante加载`.env`中的环境变量，无需明文写密钥："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8ee2948-51a8-4433-b22a-f12d8a9d8c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API 密钥加载成功：sk-你的O...\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 加载.env文件\n",
    "load_dotenv()\n",
    "\n",
    "# 获取API密钥\n",
    "open_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "langchain_api_key=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# 验证是否加载成功\n",
    "print(f\"OpenAI API 密钥加载成功：{open_api_key[:6]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36216ac-9ae7-4170-9e47-e519f37a6eb5",
   "metadata": {},
   "source": [
    "**2. 初始化Langchain核心组件**\n",
    "\n",
    "对于需要“追踪应用运行日志”使用 `LangSmith` 的场景，需要初始化Langchain的环境。\n",
    "\n",
    "- **获取 Langchain API密钥**：访问[Langchain官网](https://www.langchain.com/) 注册账号，进入“Settings →  API Keys”生成API密钥，复制到 `.env` 文件中。\n",
    "\n",
    "- 在代码中初始化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4477d90d-2efb-4be9-abff-8fbc08b6d93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 环境测试成功 ===\n",
      "调试模式状态：已开启（查看终端 DEBUG 日志）\n",
      "大模型回复： 你好！如果你想进行基于 Langchain 的环境测试，首先需要确保你已经安装了 Langchain 库及其依赖项。以下是一些基本步骤，帮助你设置和测试 Langchain 环境：\n",
      "\n",
      "1. **安装 Langchain**：\n",
      "   你可以使用 pip 来安装 Langchain。打开你的命令行工具，输入以下命令：\n",
      "   ```bash\n",
      "   pip install langchain\n",
      "   ```\n",
      "\n",
      "2. **创建一个简单的 Langchain 项目**：\n",
      "   创建一个 Python 文件，例如 `test_langchain.py`，然后在文件中导入 Langchain 并编写一些基本的代码来测试其功能。\n",
      "\n",
      "3. **示例代码**：\n",
      "   以下是一个简单的示例，展示如何使用 Langchain 进行基本的文本处理：\n",
      "   ```python\n",
      "   from langchain.chains import LLMChain\n",
      "   from langchain.llms import OpenAI\n",
      "\n",
      "   # 创建一个 OpenAI LLM 实例\n",
      "   llm = OpenAI(api_key=\"你的API密钥\")\n",
      "\n",
      "   # 创建一个链\n",
      "   chain = LLMChain(llm=llm)\n",
      "\n",
      "   # 输入一些文本并获取响应\n",
      "   response = chain.run(\"请告诉我关于人工智能的基本概念。\")\n",
      "   print(response)\n",
      "   ```\n",
      "\n",
      "4. **运行你的代码**：\n",
      "   在命令行中运行你的 Python 文件：\n",
      "   ```bash\n",
      "   python test_langchain.py\n",
      "   ```\n",
      "\n",
      "5. **检查输出**：\n",
      "   如果一切正常，你应该能看到 Langchain 返回的关于人工智能的基本概念的文本。\n",
      "\n",
      "如果你在安装或测试过程中遇到问题，欢迎随时提问！\n",
      "LangSmith 追踪：可访问 https://smith.langchain.com/ 查看详细日志\n"
     ]
    }
   ],
   "source": [
    "# 1. 导入官方推荐的基础模块（参考文档：https://python.langchain.com.cn/docs/get_started/installation）\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI  # 官方推荐：独立大模型包（0.3.x 版本标准导入路径）\n",
    "from langchain_core.messages import HumanMessage  # 官方推荐：核心消息结构（替代旧版 langchain.schema）\n",
    "\n",
    "# 2. 加载环境变量（管理 API 密钥，官方安全规范：https://python.langchain.com.cn/docs/get_started/quickstart#api-keys）\n",
    "load_dotenv()  # 从 .env 文件加载密钥，避免明文暴露\n",
    "\n",
    "# 3. 开启调试模式（官方标准方案：通过环境变量，无需导入任何 debug 模块）\n",
    "os.environ[\"LANGCHAIN_DEBUG\"] = \"true\"  # \"true\" 开启调试日志，\"false\" 关闭\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"  # 官方推荐：开启 v2 追踪（用于 LangSmith 日志）\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"我的第一个Langchain项目\"  # 官方要求：指定 LangSmith 项目名\n",
    "\n",
    "# 4. 初始化 LangSmith（参考文档：https://python.langchain.com.cn/docs/guides/tracing）\n",
    "# 注意：0.3.x 版本无需显式导入 LangSmith 类，环境变量配置后自动生效\n",
    "# 仅需确保 Langchain API 密钥已通过 .env 文件加载\n",
    "langsmith_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "if not langsmith_api_key:\n",
    "    print(\"警告：未配置 LANGCHAIN_API_KEY，LangSmith 追踪功能将无法使用\")\n",
    "\n",
    "# 5. 初始化大模型（官方示例：https://python.langchain.com.cn/docs/integrations/chat/openai）\n",
    "chat_model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",  # 官方推荐：使用稳定的模型标识（避免指定过细版本如 0.125，减少兼容性问题）\n",
    "    api_key=os.getenv(\"UIUIAPI_API_KEY\"),\n",
    "    base_url=\"https://sg.uiuiapi.com/v1\",\n",
    "    temperature=0.7,\n",
    "    max_retries=3  # 官方建议：添加重试机制，应对临时网络或限流问题\n",
    ")\n",
    "\n",
    "# 6. 官方标准测试：调用大模型验证环境（参考文档：https://python.langchain.com.cn/docs/get_started/quickstart）\n",
    "def test_official_environment():\n",
    "    try:\n",
    "        # 发送消息（官方推荐用 invoke 方法，替代旧版 __call__）\n",
    "        response = chat_model.invoke([HumanMessage(content=\"你好，基于 Langchain 中文官方文档的环境测试\")])\n",
    "        print(\"=== 环境测试成功 ===\")\n",
    "        print(\"调试模式状态：已开启（查看终端 DEBUG 日志）\")\n",
    "        print(\"大模型回复：\", response.content)\n",
    "        print(\"LangSmith 追踪：可访问 https://smith.langchain.com/ 查看详细日志\")\n",
    "    except Exception as e:\n",
    "        print(\"=== 环境测试失败 ===\")\n",
    "        print(\"错误原因：\", str(e))\n",
    "        print(\"建议参考官方文档排查：https://python.langchain.com.cn/docs/get_started/troubleshooting\")\n",
    "\n",
    "# 执行测试\n",
    "test_official_environment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1ed426-3faf-4b9b-b58e-6409bb00a1b9",
   "metadata": {},
   "source": [
    "**3. 配置代理（国内用户可选）**\n",
    "\n",
    "若国内用户调用OpenAI API时需要配置代理，在代码中添加一下配置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da76d33-de2f-4105-addf-d1b64456aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HTTP_PROXY\"]=\"http://127.0.0.1:7890\"\n",
    "os.environ[\"HTTPS_PROXY\"]=\"https://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be176ea-ec6d-48d7-8858-186dda238ba3",
   "metadata": {},
   "source": [
    "**4. 安装场景化依赖包**\n",
    "\n",
    "Langchain 本身不包含大模型、文档解析、数据库连接等功能，需安装对应\"独立子包\"或第三方依赖。\n",
    "\n",
    "官网将其分为四类：大模型集成、文档处理、向量数据库、工具调用。\n",
    "\n",
    "|功能分类|依赖包名称|用途说明|安装命令|\n",
    "|--|--|--|--|\n",
    "|**大模型集成**|`langchain-openai`|连接OpenAI或兼容OpenAI接口服务（如UIUIAPI）|`pip install langchain-openai`|\n",
    "|         |`langchain-anthropic`|连接Anthropic大模型（如Claude3）|`pip install langchain-anthropic`|\n",
    "|**文档处理**|`langchain-community`|提供PDF/Word/网页等文档加载器|`pip install langchain-community`|\n",
    "|       |`pypdf`|单独处理PDF文档（补充`langchain-community`的PDF解析能力）|`pip install pypdf`|\n",
    "|**向量数据库**|`langchain-chroma`|连接Chroma轻量级向量数据库（适合本地开发、文档检索场景）|`pip install langchain-chroma`|\n",
    "|              |`langchain-pinecone`|连接Pinecone云向量数据库（适合生产环境、大规模数据存储）|`pip install langchain-pinecone`|\n",
    "|**环境与工具**|`python-dotenv`|加载本地`.env`文件，安全管理API密钥|`pip install python-dotenv`|\n",
    "|             |`tiktoken`|OpenAI官方分词库，计算大模型调用的Token数量（避免超量）|`pip install tiktoken`|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894c5465-fe6a-4df7-a456-8a94772122ce",
   "metadata": {},
   "source": [
    "### 1.2.3 必备工具和服务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984160f8-0baf-4705-b03e-7fa91679bc2b",
   "metadata": {},
   "source": [
    "Langchain 的核心能力依赖 “外部工具 / 服务” 的支撑 —— 大模型提供 “推理能力”，数据库提供 “数据存储能力”，文档工具提供 “数据处理能力”。以下是开发前必须准备的 3 类核心工具，包含申请 / 安装步骤和使用建议：\n",
    "\n",
    "**1. 大模型服务：OpenAI API（首选）**\n",
    "\n",
    "- **注册OpenAI账号**：访问[OpenAI 官网](https://platform.openai.com/)，用邮箱注册，完成手机号验证。\n",
    "\n",
    "- **生成API密钥**：登录后进入“API Keys → Create new secret key”，输入密钥名称，点击“Create secret key”，立即复制密钥并保存到.env 文件中（密钥仅显示一次，刷新后无法再次查看）。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda89c69-78e6-463b-843a-8319f27d3d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 加载`.env`文件\n",
    "load_dotenv()\n",
    "\n",
    "chat_model = ChatOpenAI(\n",
    "    api_key=os.getenv(\"UIUIAPI_API_KEY\"),\n",
    "    base_url=\"https://sg.uiuiapi.com/v1\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7, # 生成文本的随机性(0~1，越低越严谨)\n",
    "    max_retries=3,   # 网络波动时自动重试次数\n",
    ")\n",
    "\n",
    "response = chat_model.invoke([HumanMessage(content=\"介绍 Langchain 的核心功能\")])\n",
    "print(\"================回复================\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fc92ec-d7ae-43f1-974e-96880ae7eadd",
   "metadata": {},
   "source": [
    "**2. 文档处理**\n",
    "\n",
    "虽然 `langchain-community` 提供了便捷的文件处理能力（尤其是Loader加载各种格式文件、文本分割等），但它的核心价值并不仅仅是“处理文件”，而是作为链接文件、模型、存储等组件的“中间层”，帮助大语言模型（LLM）结合实现更复杂的任务。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde48b63-9ff2-4cd1-a6ef-0ddabd785251",
   "metadata": {},
   "source": [
    "**3. 向量数据库（文档检索核心）**\n",
    "\n",
    "向量数据库用于存储\"**文档的Embedding向量**\"（将文本转化为数值向量），是实现\"**检索增强生成(RAG)**\"的关键工具（解决大模型“记不住长文档的问题”）。官方推荐入门使用Chroma，生产环境使用Pinecone。\n",
    "\n",
    "**(1) Chroma（本地轻量级向量数据库）**\n",
    "\n",
    "- **适用场景**：本地开发、小规模文档检索（如个人知识库）。\n",
    "\n",
    "- **配置示例**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8604ca05-2296-4ab3-9496-62a3e9538ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索结果： RAG 是检索增强生成技术，用于提升大模型回答准确性\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings # 用于生成文本Embedding\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 1. 初始化 Embedding 模型（将文本转化为向量）\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    api_key=os.getenv(\"UIUIAPI_API_KEY\"),\n",
    "    base_url=os.getenv(\"UIUIAPI_BASE_URL\"),\n",
    ")\n",
    "\n",
    "# 2. 初始化 Chroma 数据库（数据存储在本地 ./chroma_db 文件夹）\n",
    "\n",
    "chroma_db = Chroma(\n",
    "    collection_name=\"my_first_collection\", # 集合名（类似数据库中的表）\n",
    "    embedding_function=embedding_model, # 关联 Embedding 模型\n",
    "    persist_directory=\"./chroma_db\" # 本地存储路径\n",
    ")\n",
    "\n",
    "# 3. 向量数据库添加文档\n",
    "documents = [\n",
    "    \"Langchain 是一个 AI 应用开发框架\",\n",
    "    \"Chroma 是轻量级向量数据库\",\n",
    "    \"RAG 是检索增强生成技术，用于提升大模型回答准确性\"\n",
    "]\n",
    "chroma_db.add_texts(texts=documents, ids=[\"doc1\", \"doc2\", \"doc3\"]) # 每个文档需要唯一 ID\n",
    "\n",
    "# 4. 检索相似文档\n",
    "results = chroma_db.similarity_search(query=\"什么是RAG?\", k=1) # k=1 表示返回1个最相似结果\n",
    "print(\"检索结果：\", results[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5ea350-c3f8-4f86-aa60-0023442a2819",
   "metadata": {},
   "source": [
    "**(2) Pinecone（云向量数据库）**\n",
    "\n",
    "- **适用场景**：生产环境、大规模数据存储（如企业级知识库、多用户文档检索系统），支持高并发查询、数据持久化与多区域部署，无需本地维护服务器。\n",
    "\n",
    "- **配置前提**：\n",
    "\n",
    "    - 访问 [Pinecone 官网](https://www.pinecone.io/) 注册账号，创建项目并创建Index\n",
    "\n",
    "    - 在 `.env` 文件中补充配置： `PINECONE_API_KEY`\n",
    " \n",
    "    - 安装官方适配包：`pip install pinecone`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd98d39-fdb3-4dd4-bae0-30f342f02cf5",
   "metadata": {},
   "source": [
    "**初始化**\n",
    "\n",
    "方式1：通过环境变量自动加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2c41cae-cc11-439c-8979-97e6690ceba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有索引名称： ['personal-index']\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pinecone import Pinecone\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 自动从环境变量 PINECONE_API_KEY 读取密钥\n",
    "pc = Pinecone()\n",
    "\n",
    "# 验证初始化：列出所有索引\n",
    "print(\"所有索引名称：\", pc.list_indexes().names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb3459-4ae5-45ae-8fa7-b38ecbdfedb0",
   "metadata": {},
   "source": [
    "方式2：显式传入参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279bccb4-288f-4c4b-86dd-603cb11ddabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pinecone import Pinecone\n",
    "from urllib3 import make_headers\n",
    "\n",
    "pc = Pinecone(\n",
    "    api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
    "    proxy_url=\"https://your-enterprise-proxy.com\",  # 企业代理地址（可选）\n",
    "    proxy_headers=make_headers(proxy_basic_auth=\"username:password\"),  # 代理认证（可选）\n",
    "    ssl_ca_certs=\"path/to/enterprise-ca.pem\",  # 自定义SSL证书（可选，企业内网常用）\n",
    "    ssl_verify=True,  # 启用SSL验证（生产环境必须开启，默认True）\n",
    "    additional_headers={\"X-Custom-Header\": \"your-value\"},  # 自定义请求头（可选）\n",
    "    pool_threads=4  # 连接池线程数（并发请求时可适当增大）\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527b85f6-b83c-43fe-a056-2bfe38b2cd50",
   "metadata": {},
   "source": [
    "**索引管理**\n",
    "\n",
    "- **创建索引**：支持两种部署模式，需指定向量维度（需要与嵌入模型维度一致，如OpenAI为1536）、相似度metric函数(cosine/dotproduct/euclidean)\n",
    "\n",
    "    - 模式1：无服务器索引（适合中小规模）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a30b46-a84b-4873-b206-d9e584d48e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone()\n",
    "\n",
    "pc.create_index(\n",
    "    name=\"serverless-doc-index\",\n",
    "    dimension=1536,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-west-2\"\n",
    "    ),\n",
    "    deletion_protection=\"enabled\" # 启动删除保护（避免误删，生产建议开启）\n",
    ")\n",
    "\n",
    "while pc.describe_index(\"serverless-doc-index\").status[\"ready\"] is False:\n",
    "    import time\n",
    "    time.sleep(5)\n",
    "print(\"索引已就绪\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae67d961-88f4-4c05-aa6f-d999dd5990a8",
   "metadata": {},
   "source": [
    "        - 模式2：Pod索引（适合大规模生产）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb49d0c-a8b2-42af-8378-c29130c0f443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, PodSpec\n",
    "\n",
    "pc = Pinecone()\n",
    "\n",
    "# 创建Pod索引（参考帮助文档示例）\n",
    "pc.create_index(\n",
    "    name=\"pod-production-index\",\n",
    "    dimension=1536,\n",
    "    metric=\"cosine\",\n",
    "    spec=PodSpec(\n",
    "        environment=\"us-east1-gcp\",  # Pod环境（需与云厂商区域匹配）\n",
    "        pod_type=\"p1.x1\",  # Pod类型：p1（通用）/s1（高性能），x1表示规格\n",
    "        replicas=3  # 副本数（越多可用性越高，生产建议≥2）\n",
    "    ),\n",
    "    deletion_protection=\"enabled\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6bad5e-586c-4637-9f4e-61e412883fd1",
   "metadata": {},
   "source": [
    "- **索引状态查询**(describe_index()/has_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c86e148-6063-4823-b267-9079366949ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "索引名：personal-index\n",
      "维度：1024\n",
      "Host: personal-index-fm8epwm.svc.aped-4627-b74a.pinecone.io\n",
      "状态： 就绪\n",
      "--------------------------------------------------\n",
      "personal-index索引信息： {'deletion_protection': 'disabled',\n",
      " 'dimension': 1024,\n",
      " 'host': 'personal-index-fm8epwm.svc.aped-4627-b74a.pinecone.io',\n",
      " 'metric': 'cosine',\n",
      " 'name': 'personal-index',\n",
      " 'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
      " 'status': {'ready': True, 'state': 'Ready'}}\n",
      "成功连接到索引 personal-index \n",
      "索引信息： {'dimension': 1024,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pinecone import Pinecone\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "index_name=os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "embedding_dimension=os.getenv(\"PINECONE_DIM\")\n",
    "metric=os.getenv(\"PINECONE_METRIC\")\n",
    "\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "# 列出所有索引并遍历详情\n",
    "\n",
    "for i in pc.list_indexes():\n",
    "    print(f\"索引名：{i.name}\")\n",
    "    print(f\"维度：{i.dimension}\")\n",
    "    print(f\"Host: {i.host}\")\n",
    "    print(f\"状态： {\"就绪\" if i.status['ready'] else \"创建中\"}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "# 查看特定索引信息\n",
    "print(f\"{index_name}索引信息：\", pc.describe_index(index_name))\n",
    "\n",
    "if pc.has_index(index_name):\n",
    "    index = pc.Index(index_name)\n",
    "    print(f\"成功连接到索引 {index_name} \\n索引信息： {index.describe_index_stats()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cdec82-74b1-4522-9f74-cfe3a2f2b5d7",
   "metadata": {},
   "source": [
    "- **索引调整**(`configure_index()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f75c5d43-abda-4e0a-996f-42a21716ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.configure_index(\n",
    "    name=os.getenv(\"PINECONE_INDEX_NAME\"),\n",
    "    deletion_protection=\"enabled\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6445e518-6e9b-47b3-8dbc-7c141c80fddb",
   "metadata": {},
   "source": [
    "**数据操作（通过`Index`对象）**\n",
    "\n",
    "通过 `pc.Index()` 获取索引操作对象后，可执行向量插入(upsert)、查询（query）等操作。\n",
    "\n",
    "1. 获取`Index`对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42473b04-c1af-4dc5-bc0a-ce82287e4e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://personal-index-fm8epwm.svc.aped-4627-b74a.pinecone.io\n"
     ]
    }
   ],
   "source": [
    "# 方式一：通过Host连接（生产环境推荐）\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pinecone import Pinecone\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 自动从环境变量 PINECONE_API_KEY 读取密钥\n",
    "pc = Pinecone()\n",
    "\n",
    "index_host = os.getenv(\"PINECONE_HOST\")\n",
    "print(index_host)\n",
    "index = pc.Index(\n",
    "    host=index_host,\n",
    "    pool_threads=8 # 并发插入时增大线程数，提升效率\n",
    ")\n",
    "\n",
    "# 方式二：通过索引名连接（测试用，生产环境不推荐）\n",
    "# 会额外调用控制平面获取Host，增加延迟\n",
    "# index = pc.Index(name=os.getenv(\"PINECONE_INDEX_NAME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c36dc46-9b74-4743-8aff-c62e093779ae",
   "metadata": {},
   "source": [
    "2. 向量插入(`upsert()`)\n",
    "\n",
    "由于我的这个PineCone向量数据库使用的Embedding模型是`llama-text-embed-v2`因此，要严格遵守向量的编码加载方式。\n",
    "\n",
    "安装pytorch： `conda install pytorch torchvision -c conda-forge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c75fac-4468-4cdb-aa8b-dac07f6d3622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例1：批量插入（推荐，减少请求次数）\n",
    "vectors = [\n",
    "    # (向量ID, 向量值, 元数据)：元数据可存储文本、时间等附加信息\n",
    "    (\"doc-1\", [0.12, 0.34, 0.56, ..., 0.1536], {\"title\": \"Pinecone官方SDK文档\", \"type\": \"tech\"}),\n",
    "    (\"doc-2\", [0.78, 0.90, 0.23, ..., 0.1536], {\"title\": \"向量检索原理\", \"type\": \"theory\"}),\n",
    "    # 可添加更多向量（单次最大1000条）\n",
    "]\n",
    "\n",
    "# 执行插入（返回插入结果）\n",
    "upsert_result = index.upsert(\n",
    "    vectors=vectors,\n",
    "    namespace=\"2024-q3-docs\"  # 命名空间（可选，用于数据隔离，如按季度划分）\n",
    ")\n",
    "print(f\"成功插入向量数：{upsert_result['upserted_count']}\")\n",
    "\n",
    "# 示例2：单条插入（适合实时新增数据）\n",
    "index.upsert(\n",
    "    vectors=[(\"doc-3\", [0.45, 0.67, 0.89, ..., 0.1536], {\"title\": \"生产环境部署指南\", \"type\": \"tech\"})],\n",
    "    namespace=\"2024-q3-docs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e949675-6d92-4d46-8de0-d00b84c437e5",
   "metadata": {},
   "source": [
    "3. 向量查询（`query()`）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b02a13f-a1c0-49e7-8d12-34372c606f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 准备查询向量（需与索引维度一致，实际场景由嵌入模型生成）\n",
    "query_vector = [0.11, 0.33, 0.55, ..., 0.1536]  # 示例：\"Pinecone SDK使用\"的向量\n",
    "\n",
    "# 2. 执行查询（参考帮助文档参数）\n",
    "query_result = index.query(\n",
    "    vector=query_vector,\n",
    "    top_k=3,  # 返回Top 3相似向量\n",
    "    include_metadata=True,  # 返回元数据（如标题、类型）\n",
    "    namespace=\"2024-q3-docs\",  # 仅查询指定命名空间\n",
    "    filter={\"type\": {\"$eq\": \"tech\"}}  # 过滤条件：仅返回类型为tech的结果（可选）\n",
    ")\n",
    "\n",
    "# 3. 解析查询结果\n",
    "print(\"查询到的相似向量：\")\n",
    "for match in query_result[\"matches\"]:\n",
    "    print(f\"向量ID：{match['id']}\")\n",
    "    print(f\"相似度得分：{match['score']:.4f}\")  # 得分越高越相似（cosine范围0-1）\n",
    "    print(f\"元数据：{match['metadata']}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1a2dc0-8b3e-4040-8c9d-7d9a05e6cdec",
   "metadata": {},
   "source": [
    "## 1.3 基本使用示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8709e5d2-ad82-4b91-a66b-818bab5d6ba4",
   "metadata": {},
   "source": [
    "### 1.3.1 编写一个Langchain应用：LLM基础对话与提示词模板"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41530867-4149-4cf8-b6d3-250fd9bf7f1e",
   "metadata": {},
   "source": [
    "第一个应用将聚焦Langchain最核心的“LLM调用”与“提示词模板(Prompt Template)”能力 ———— 通过模板同一对话格式，调用大模型生成结构化相应，适用于客服对话、信息问答等基础场景。\n",
    "\n",
    "**(1) 前置准备**\n",
    "\n",
    "安装Langchain的依赖，参考前文\n",
    "\n",
    "**(2) 第一个应用：结构化问答机器人**\n",
    "\n",
    "需求：创建一个“Python 学习助手”，接收用户关于Python的问题后，按“问题重述 → 核心知识点 → 示例代码” 的固定格式生成回答， 避免LLM 输出混乱。\n",
    "\n",
    "- 步骤一：加载环境变量与初始化LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4de351-fbb1-401d-afa9-c6c429bfa44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "load_dotenv()  # 从 .env 文件加载密钥，避免明文暴露\n",
    "\n",
    "# 开启调试模式\n",
    "os.environ[\"LANGCHAIN_DEBUG\"] = \"true\"  # \"true\" 开启调试日志，\"false\" 关闭\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"  # 官方推荐：开启 v2 追踪（用于 LangSmith 日志）\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"我的第一个Langchain项目\"  # 官方要求：指定 LangSmith 项目名\n",
    "\n",
    "# 4. 初始化 LangSmith（参考文档：https://python.langchain.com.cn/docs/guides/tracing）\n",
    "# 注意：0.3.x 版本无需显式导入 LangSmith 类，环境变量配置后自动生效\n",
    "# 仅需确保 Langchain API 密钥已通过 .env 文件加载\n",
    "langsmith_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "if not langsmith_api_key:\n",
    "    print(\"警告：未配置 LANGCHAIN_API_KEY，LangSmith 追踪功能将无法使用\")\n",
    "\n",
    "# 5. 初始化大模型（官方示例：https://python.langchain.com.cn/docs/integrations/chat/openai）\n",
    "chat_model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",  # 官方推荐：使用稳定的模型标识（避免指定过细版本如 0.125，减少兼容性问题）\n",
    "    api_key=os.getenv(\"UIUIAPI_API_KEY\"),\n",
    "    base_url=\"https://sg.uiuiapi.com/v1\",\n",
    "    temperature=0.7,\n",
    "    max_retries=3  # 官方建议：添加重试机制，应对临时网络或限流问题\n",
    ")\n",
    "\n",
    "# 6. 官方标准测试：调用大模型验证环境（参考文档：https://python.langchain.com.cn/docs/get_started/quickstart）\n",
    "def test_official_environment():\n",
    "    try:\n",
    "        # 发送消息（官方推荐用 invoke 方法，替代旧版 __call__）\n",
    "        response = chat_model.invoke([HumanMessage(content=\"你好，基于 Langchain 中文官方文档的环境测试\")])\n",
    "        print(\"=== 环境测试成功 ===\")\n",
    "        print(\"大模型回复：\", response.content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"=== 环境测试失败 ===\")\n",
    "        print(\"错误原因：\", str(e))\n",
    "        print(\"建议参考官方文档排查：https://python.langchain.com.cn/docs/get_started/troubleshooting\")\n",
    "\n",
    "# 执行测试\n",
    "test_official_environment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10e484b-c891-4f58-a89b-fc737d1b614b",
   "metadata": {},
   "source": [
    "**步骤二：定义提示词模板（Prompt Template）**\n",
    "\n",
    "用 `PromptTemplate` 固定回答格式，通过 `{user_question}` 占位符接收用户输入，确保 LLM 按规则输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89cd274d-2252-4881-a67d-be43f92be9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "你是一个专业的Python学习助手，请按以下格式回答用户关于Python的问题：\n",
    "1. 问题重述：用一句话概况用户的问题；\n",
    "2. 核心知识点：列出回答该问题需要用到的Python核心概念或语法；\n",
    "3. 示例代码：提供1~2个可运行的Python代码示例，并添加注释；\n",
    "\n",
    "用户的问题：{user_question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"user_question\"] # 与模板中的 {user_question} 对应\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65701094-1169-44b5-ae32-58508f228e00",
   "metadata": {},
   "source": [
    "**步骤三：创建并行LLM链（LLM Chain）**\n",
    "\n",
    "Langchain中的链(Chain)用于串联子模块的流程，避免手动拼接字符串和调用LLM："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a5bee-1fd9-4df8-9926-7fa6a712b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不再需要导入LLMChain\n",
    "# 直接使用 prompt 与 llm 组成管道\n",
    "chain = prompt | chat_model\n",
    "\n",
    "user_question = \"如何用Python读取CSV文件并筛选出某一列值大于100的行?\"\n",
    "response = chain.invoke({\"user_question\": user_question})  # 注意需要传入字典参数\n",
    "content = response.content\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba34c013-81d0-4121-9bfa-6f161f12a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "stream_chain = prompt | chat_model\n",
    "\n",
    "accumulated_content = \"\"\n",
    "\n",
    "for chunk in stream_chain.stream({\"user_question\": user_question}):\n",
    "    current_chunk = chunk.content.strip() # 去除空字符串，避免无效步骤\n",
    "\n",
    "    if current_chunk:\n",
    "        accumulated_content += current_chunk\n",
    "        print(accumulated_content, end=\"\\r\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9deccf0-39f5-465f-9979-a7240ac17821",
   "metadata": {},
   "source": [
    "### 1.3.2 基础文档处理：加载本地文档并实现问答"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8b0f7-8f2c-4405-b544-589b08fb878d",
   "metadata": {},
   "source": [
    "Langchain对“文档处理”有成熟的支持，可轻松加载本地TXT/Markdown/PDF文档：**文档分割 → 向量存储 → 检索问答** 实现基于现有文档的问答。（如企业知识问答、PDF文档解读）。\n",
    "\n",
    "本节以“加载本地TXT文档，回答文档想问问题”为例，演示完整流程。\n",
    "\n",
    "**1. 前置准备**\n",
    "\n",
    "- 准备本地文档：创建 python_basics.txt 文件，写入少量 Python 基础内容（示例内容如下）；\n",
    "\n",
    "- 安装文档处理依赖：需安装 pypdf（处理 PDF）、langchain-community（加载本地文档）、pinecone（向量存储，复用前文的 Pinecone 环境）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50463171-4e9c-49fb-aabe-765073905192",
   "metadata": {},
   "source": [
    "**2. 文档处理完整流程：加载 → 分割 → 向量存储 → 检索问答**\n",
    "\n",
    "**步骤1：加载本地文档**\n",
    "\n",
    "使用`Langchain`的`TextLoader`加载TXT文档（若为PDF，需PyPDFLoader）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c94c4ad9-ed33-40ef-b67a-af916fd1d9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载的文档页数：1\n",
      "文档内容预览：# Python 基础语法总结\n",
      "1. 变量定义：Python 中无需声明变量类型，直接赋值即可，如 x = 10（整数）、name = \"LangChain\"（字符串）。\n",
      "2. 条件语句：使用 if-elif-else 结构，注意缩进（4个空格），如：\n",
      "   if x > 0:\n",
      "       print(\"x 是正数\")\n",
      "   elif x == 0:\n",
      "       print(\"x 是零\")\n",
      " ...\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"python_basics.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load() # 加载文档，返回 Document 对象列表（每个对象包含 page_content 和 metadata）\n",
    "print(f\"加载的文档页数：{len(documents)}\")\n",
    "print(f\"文档内容预览：{documents[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003ed94-5093-47a3-9d9a-305a3ebb0c97",
   "metadata": {},
   "source": [
    "**步骤2：分割文档（避免向量长度超限）**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d19f6d5-5b76-4272-a248-42e3b8575f2a",
   "metadata": {},
   "source": [
    "LLM和向量模型对输入长度有上限（如OpenAI Embedding 支持最大8191 token），需将长文档按固定长度分割为\"**文档块(Chunks)**\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9546ea52-af0d-49bf-bad7-bc37aec6d44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分割后的文档块数量：3\n",
      "第一个文档块内容：# Python 基础语法总结\n",
      "1. 变量定义：Python 中无需声明变量类型，直接赋值即可，如 x = 10（整数）、name = \"LangChain\"（字符串）。\n",
      "2. 条件语句：使用 if-elif-else 结构，注意缩进（4个空格），如：\n",
      "   if x > 0:\n",
      "       print(\"x 是正数\")\n",
      "   elif x == 0:\n",
      "       print(\"x 是零\")\n",
      "   else:\n",
      "       print(\"x 是负数\")\n",
      "3. 循环语句：for 循环用于遍历序列（如列表、字符串），while 循环用于条件循环。示例：\n",
      "   # for 循环遍历列表\n",
      "第二个文档块内容：# for 循环遍历列表\n",
      "   fruits = [\"apple\", \"banana\", \"cherry\"]\n",
      "   for fruit in fruits:\n",
      "       print(fruit)\n",
      "   # while 循环计算 1~10 的和\n",
      "   sum = 0\n",
      "   i = 1\n",
      "   while i <= 10:\n",
      "       sum += i\n",
      "       i += 1\n",
      "   print(sum)  # 输出 55\n",
      "4. 函数定义：使用 def 关键字，如：\n",
      "   def add(a, b):\n",
      "       return a + b\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300, # 每个文档块的最大字符数\n",
    "    chunk_overlap=50, # 相邻文档块的重叠字符数（保留上下文关联）\n",
    "    length_function=len  # 按字符数计算长度\n",
    ")\n",
    "\n",
    "# 执行分割，得到文档块列表\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "print(f\"分割后的文档块数量：{len(split_documents)}\")\n",
    "print(f\"第一个文档块内容：{split_documents[0].page_content}\")\n",
    "print(f\"第二个文档块内容：{split_documents[1].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ba175-85a2-41a4-9faa-434e804f2215",
   "metadata": {},
   "source": [
    "**步骤3：将文档快存入向量数据库（Chroma）**\n",
    "\n",
    "将分割后的文档通过`OpenAI Embeddings`生成向量，存入 `Chroma` 向量库（便于后序检索相似内容）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7618b9cf-4ed8-4039-9d05-4ec902ad52c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=os.getenv(\"UIUIAPI_API_KEY\"),\n",
    "    base_url=os.getenv(\"UIUIAPI_BASE_URL\"),\n",
    "    model=\"text-embedding-3-small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e2fec7c-5ae3-4e54-a3ac-59ddb1ed1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化Chroma向量存储，并将向量存入\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=split_documents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862d60c4-d95f-49cf-a7ff-0cdc8caad6a9",
   "metadata": {},
   "source": [
    "**步骤4：创建检索问答链（基于文档回答问题）**\n",
    "\n",
    "通过`RetrievalQA`链串联“**向量检索 → LLM生成回答**”流程，确保LLM仅基于加载的本地文档回答问题（避免编造信息）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f49d2eb-ee19-45b2-bcf7-dea3d560338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chat_model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",  # 官方推荐：使用稳定的模型标识（避免指定过细版本如 0.125，减少兼容性问题）\n",
    "    api_key=os.getenv(\"UIUIAPI_API_KEY\"),\n",
    "    base_url=os.getenv(\"UIUIAPI_BASE_URL\"),\n",
    "    temperature=0.2,\n",
    "    max_retries=3  # 官方建议：添加重试机制，应对临时网络或限流问题\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    chain_type=\"stuff\", # 简单场景推荐用 \"stuff\"：将所有检索到的文档块传入 LLM\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2}),  # 检索 top-2 相似文档\n",
    "    return_source_documents=True  # 返回回答对应的源文档（便于验证回答来源）\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9738eff6-531f-466c-a2ed-1990ab5ab4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "基于文档的回答：\n",
      "要在 Python 中定义一个函数并调用，可以按照以下步骤进行：\n",
      "\n",
      "1. 使用 `def` 关键字定义函数，指定函数名和参数列表，然后在缩进块内编写函数体。\n",
      "2. 在函数体内编写函数的具体逻辑，可以包括各种操作和返回值。\n",
      "3. 调用函数时，直接使用函数名和参数列表进行调用。\n",
      "\n",
      "例如，定义一个简单的函数 `add` 来计算两个数的和，并调用该函数：\n",
      "\n",
      "```python\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "\n",
      "# 调用函数\n",
      "result = add(3, 5)\n",
      "print(result)  # 输出 8\n",
      "```\n",
      "\n",
      "在这个例子中，我们定义了一个函数 `add`，接受两个参数 `a` 和 `b`，返回它们的和。然后我们调用这个函数，并将结果打印出来。\n",
      "\n",
      "回答来源的文档块：\n",
      "1. # Python 基础语法总结\n",
      "1. 变量定义：Python 中无需声明变量类型，直接赋值即可，如 x = 10（整数）、name = \"LangChain\"（字符串）。\n",
      "2. 条件语句：使用 if-elif-else 结构，注意缩进（4个空格），如：\n",
      "   if x > 0:\n",
      "       print(\"x 是正数\")\n",
      "   elif x == 0:\n",
      "       print(\"x 是零\")\n",
      "   else:\n",
      "       print(\"x 是负数\")\n",
      "3. 循环语句：for 循环用于遍历序列（如列表、字符串），while 循环用于条件循环。示例：\n",
      "   # for 循环遍历列表\n",
      "2. # for 循环遍历列表\n",
      "   fruits = [\"apple\", \"banana\", \"cherry\"]\n",
      "   for fruit in fruits:\n",
      "       print(fruit)\n",
      "   # while 循环计算 1~10 的和\n",
      "   sum = 0\n",
      "   i = 1\n",
      "   while i <= 10:\n",
      "       sum += i\n",
      "       i += 1\n",
      "   print(sum)  # 输出 55\n",
      "4. 函数定义：使用 def 关键字，如：\n",
      "   def add(a, b):\n",
      "       return a + b\n"
     ]
    }
   ],
   "source": [
    "user_question = \"如何用 Python 定义一个函数并调用？\"\n",
    "result = qa_chain.invoke({\"query\": user_question})\n",
    "\n",
    "# 打印结果\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"基于文档的回答：\")\n",
    "print(result[\"result\"])\n",
    "print(\"\\n回答来源的文档块：\")\n",
    "for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "    print(f\"{i}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c2437-0b59-402e-847d-993be578a796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
