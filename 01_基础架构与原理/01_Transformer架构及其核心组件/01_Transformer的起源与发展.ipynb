{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b043b574-193f-49f0-b543-6731fefa6d2b",
   "metadata": {},
   "source": [
    "# Transformer的起源与发展"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177734cd-fa72-43ce-bddc-eb4f472c0410",
   "metadata": {},
   "source": [
    "这一节从 传统序列模型的不足 → Attention 的提出 → Transformer 的诞生 → 大模型化与多模态发展，形成完整的历史脉络，体现研究演化逻辑，并辅以核心论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed974983-296c-42ce-ae66-733a14b8dad9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. 目录"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a79f204-bc29-4d7a-97fa-3d897ce3e74d",
   "metadata": {},
   "source": [
    "1.1 传统神经网络模型的局限性\n",
    "\n",
    "- RNN / LSTM 的提出与优势\n",
    "    - RNN 在处理序列数据中的重要性。\n",
    "    - LSTM / GRU 的改进（长距离依赖问题）。\n",
    "\n",
    "- 局限性\n",
    "    - 计算难以并行化。\n",
    "    - 长序列建模能力有限。\n",
    "    - 梯度消失 / 梯度爆炸问题。\n",
    "\n",
    "- 参考文献\n",
    "    - Hochreiter & Schmidhuber, 1997 （LSTM）\n",
    "    - Cho et al., 2014 （GRU）\n",
    "\n",
    "1.2 基于卷积的序列建模探索\n",
    "\n",
    "- CNN 在 NLP 任务中的应用\n",
    "\n",
    "    - 卷积捕捉局部依赖的能力。\n",
    "\n",
    "- 局限性\n",
    "\n",
    "    - 缺乏对全局依赖的建模能力。\n",
    "\n",
    "- 参考文献\n",
    "\n",
    "    - Kalchbrenner et al., 2014 （CNN for NLP）\n",
    "\n",
    "    - Gehring et al., 2017 （Convolutional Sequence to Sequence Learning）\n",
    " \n",
    "1.3 Seq2Seq 与 Attention 机制的兴起\n",
    "\n",
    "- Seq2Seq 框架的提出\n",
    "\n",
    "    - Encoder-Decoder 架构在机器翻译中的应用。\n",
    "\n",
    "- Attention 机制的引入\n",
    "\n",
    "    - 解决长距离依赖问题。\n",
    "\n",
    "    - 提升翻译与生成效果。\n",
    "\n",
    "- 参考文献\n",
    "\n",
    "    - Sutskever et al., 2014 （Sequence to Sequence Learning with Neural Networks）\n",
    "\n",
    "    - Bahdanau et al., 2014 （Neural Machine Translation by Jointly Learning to Align and Translate）\n",
    "\n",
    "    - Luong et al., 2015 （Effective Approaches to Attention-based Neural Machine Translation）\n",
    "\n",
    "1.4 Transformer 的提出\n",
    "\n",
    "- 核心创新\n",
    "\n",
    "    - 完全基于 Self-Attention 的架构。\n",
    "\n",
    "    - 去掉循环与卷积，提升并行性与效率。\n",
    "\n",
    "    - Multi-Head Attention、位置编码、残差连接等关键组件。\n",
    "\n",
    "- 论文贡献\n",
    "\n",
    "    - 提出“Attention is All You Need”，引发 NLP 架构范式转变。\n",
    "\n",
    "- 参考文献\n",
    "\n",
    "    - Vaswani et al., 2017 （Attention Is All You Need）\n",
    "\n",
    "1.5 Transformer 在后续模型中的发展\n",
    "\n",
    "- BERT 系列\n",
    "\n",
    "    - 双向 Transformer 预训练（Masked Language Modeling + NSP）。\n",
    "\n",
    "- GPT 系列\n",
    "\n",
    "    - 单向自回归 Transformer，推动大规模语言模型发展。\n",
    "\n",
    "- T5、BART 等 Seq2Seq Transformer\n",
    "\n",
    "    - 统一的文本到文本框架，广泛应用于多任务学习。\n",
    "\n",
    "- 参考文献\n",
    "\n",
    "    - Devlin et al., 2018 （BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding）\n",
    "\n",
    "    - Radford et al., 2018 （Improving Language Understanding by Generative Pre-Training, GPT）\n",
    "\n",
    "    - Lewis et al., 2019 （BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension）\n",
    "\n",
    "    - Raffel et al., 2020 （Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, T5）\n",
    "\n",
    "1.6 大规模 Transformer 模型与应用扩展\n",
    "\n",
    "- 大规模化\n",
    "\n",
    "    - 参数量的急剧增长（GPT-3、PaLM、LLaMA）。\n",
    "\n",
    "- 跨模态扩展\n",
    "\n",
    "    - 多模态 Transformer（CLIP, Flamingo, Gemini 等）。\n",
    "\n",
    "- 行业应用\n",
    "\n",
    "    - 在搜索、推荐、智能助理等方面的突破。\n",
    "\n",
    "- 参考文献\n",
    "\n",
    "    - Brown et al., 2020 （Language Models are Few-Shot Learners, GPT-3）\n",
    "\n",
    "    - Chowdhery et al., 2022 （PaLM: Scaling Language Modeling with Pathways）\n",
    "\n",
    "    - Touvron et al., 2023 （LLaMA: Open and Efficient Foundation Language Models）\n",
    "\n",
    "    - Radford et al., 2021 （Learning Transferable Visual Models From Natural Language Supervision, CLIP）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e33f9-51d7-4e48-a4d3-d202d2b2ae4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.1 传统神经网络模型的局限性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7b1d1-bb32-44ad-8c5a-0b3c24a936cb",
   "metadata": {},
   "source": [
    "### 1.1.1 RNN / LSTM 的提出与优势"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc74eac-90e3-4b03-ba3e-950487b89c5e",
   "metadata": {},
   "source": [
    "**RNN 在处理序列数据中的重要性**\n",
    "\n",
    "在自然语言处理（NLP）、语音识别、机器翻译等任务中，序列数据非常常见。序列数据是一种有顺序关系的数据类型，例如一段话中的单词顺序、音频文件中的声音波形等。传统的神经网络（如全连接网络）在处理序列数据时，无法保留之前信息的顺序，这限制了它们在序列建模中的应用。\n",
    "\n",
    "为了解决这一问题，**循环神经网络**（**RNN，Recurrent Neural Network**）应运而生。RNN通过一个循环结构，使得网络能够保留前一步的输出并将其作为当前输入的一部分，从而能够处理具有时间依赖的序列数据。这样，RNN就能根据输入数据的顺序来做出判断，这对于语言、语音等**有序数据**的处理尤为重要。\n",
    "\n",
    "**LSTM / GRU 的改进（长距离依赖问题）**\n",
    "\n",
    "尽管RNN能够处理序列数据，但它在长序列中存在一些问题，特别是长距离依赖（Long-Term Dependencies）。假设一个序列中的某些重要信息在前面很早的位置，而RNN需要依赖前面的信息来进行后续的决策，这时网络可能会面临**梯度消失**（**Vanishing Gradient**）或**梯度爆炸**（**Exploding Gradient**）的问题。\n",
    "\n",
    "为了应对这个问题，**长短期记忆网络**（**LSTM，Long Short-Term Memory**）应运而生。LSTM通过引入一种“门控”机制，能够控制信息的“遗忘”与“记忆”，使得网络能够有效地捕捉长期依赖关系。LSTM使得神经网络可以处理更长时间跨度的信息，从而大大改善了传统RNN的性能。\n",
    "\n",
    "与LSTM类似，**门控循环单元**（**GRU，Gated Recurrent Unit**）也是为了解决同样的问题而提出的。GRU在结构上比LSTM稍微简单一些，它同样通过门控机制来控制信息的流动，但相较于LSTM，它省略了一些细节，效率更高，但在某些情况下表现与LSTM相似。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38ce0d8-2568-41ea-ad5d-8ce8958050d1",
   "metadata": {},
   "source": [
    "### 1.1.2 局限性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e76ff14-13e1-4d89-8807-fff270056833",
   "metadata": {},
   "source": [
    "虽然RNN、LSTM和GRU在序列数据处理中取得了显著的成果，但它们依然存在一些难以克服的局限性，这些问题在实际应用中往往会影响它们的效率和效果。\n",
    "\n",
    "**(1) 计算难以并行化**\n",
    "\n",
    "RNN的计算需要依赖前一个时间步的输出作为当前时间步的输入，这就使得网络的计算过程是顺序的，而无法进行并行计算。尤其在面对长序列时，计算时间可能会变得非常漫长，这对于现代深度学习应用中的效率要求是一个重大障碍。\n",
    "\n",
    "**(2) 长序列建模能力有限**\n",
    "\n",
    "尽管LSTM和GRU通过其门控机制改善了长距离依赖问题，但它们依然存在一定的限制。当序列非常长时，模型需要通过多次迭代来逐步捕捉信息，这种方式会使得模型的记忆能力逐渐衰减，导致长期依赖问题没有得到完全解决。\n",
    "\n",
    "**(3) 梯度消失 / 梯度爆炸问题**\n",
    "\n",
    "在训练RNN、LSTM、GRU时，模型的权重参数会通过反向传播进行更新。然而，由于反向传播过程中的梯度会随着层数的增加而逐渐减小（梯度消失）或者增大（梯度爆炸），这导致了深层网络训练的困难。尤其是在处理长序列时，这个问题更为明显，会导致网络训练不稳定，影响最终模型的效果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136788b2-9e79-48e3-ad64-486b31e7dfc8",
   "metadata": {},
   "source": [
    "### 1.1.3 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24cc187-79aa-44df-b3a8-d5382564d02d",
   "metadata": {},
   "source": [
    "- Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation, 9(8), 1735–1780.\n",
    "\n",
    "- Cho, K., Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. Proceedings of the EMNLP 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41348bcb-de83-4282-bc9c-6f205b964728",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.2 基于卷积的序列建模探索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47fdd6c-8ab4-4cf5-a808-525e04a9ceeb",
   "metadata": {},
   "source": [
    "**1.2.1 CNN 在 NLP 任务中的应用**\n",
    "\n",
    "在处理序列数据时，**卷积神经网络**（**CNN，Convolutional Neural Networks**）常常被用来替代传统的循环神经网络（RNN）进行特征提取，尤其是在 **自然语言处理**（NLP） 任务中。卷积网络最初被设计用于**图像处理**，其主要优势在于能够高效地从**局部区域提取特征**。在 NLP 任务中，卷积网络被用来捕捉文本中的 **局部依赖关系**，例如，识别一段文本中**连续单词**或**字符之间的关系**。\n",
    "\n",
    "在NLP应用中，卷积操作 **通过滑动窗口的方式**，在输入的文本序列上进行局部区域的扫描。这种方式能够通过多个卷积核同时提取不同的特征，使得模型在处理长文本时，依然能够保留文本中的局部信息。例如，卷积网络被广泛应用于 **文本分类**、**情感分析**、**命名实体识别**（NER） 等任务。\n",
    "\n",
    "- **卷积捕捉局部依赖的能力**\n",
    "\n",
    "CNN非常适合捕捉 **局部依赖**，这一点在处理文本时尤为重要。在处理句子或段落时，许多语言现象仅限于局部范围内的依赖关系，例如**词语顺序的变化对语义的影响**、**同义词的使用**等。这些依赖通常出现在邻近的单词之间，CNN的卷积核能够通过滑动窗口有效地捕捉这些局部模式。\n",
    "\n",
    "具体来说，在文本分类任务中，CNN通过提取不同长度的n-gram（例如连续3个词、5个词的组合）来理解词与词之间的关系。这种方式能够有效地识别出文本中的短期特征，例如情感极性、主题词等，从而为下游任务提供有效的特征表示。\n",
    "\n",
    "**1.2.2 局限性**\n",
    "\n",
    "尽管卷积神经网络在文本任务中表现出了显著的优势，但它也存在一些不可忽视的局限性，特别是在捕捉 **全局依赖关系** 方面。\n",
    "\n",
    "**(1) 缺乏对全局依赖的建模能力**\n",
    "\n",
    "虽然卷积网络在局部区域内的表现非常出色，但它们通常并不擅长处理 **长距离依赖**。在文本中，很多重要的信息可能并不集中在某一小段序列内，而是分布在整个文本中。例如，**长篇文章中的前后句子之间可能有深层的语义联系**，而CNN仅通过局部卷积来处理信息，无法有效捕捉到远距离单词之间的关系。\n",
    "\n",
    "为了弥补这一点，研究者们提出了 **层叠卷积**（**stacked convolutions**）或者使用**更大的卷积核**来覆盖更长的文本范围，然而这种方法虽然可以**扩展感受野**，但它仍然无法像 RNN 或 Transformer 那样**灵活**地建模长距离的依赖关系。\n",
    "\n",
    "此外，卷积操作的局部感知限制也使得其难以处理长文本中的跨句依赖。这一问题在 **机器翻译** 或 **文本生成** 等任务中尤为突出。\n",
    "\n",
    "**1.2.3 参考文献**\n",
    "\n",
    "- Kalchbrenner, N., Grefenstette, E., & Blunsom, P. (2014). A Convolutional Neural Network for Modelling Sentences. In Proceedings of the ACL 2014.\n",
    "\n",
    "- Gehring, J., Auli, M., Grangier, D., Yarats, D., & Dauphin, Y. (2017). Convolutional Sequence to Sequence Learning. In Proceedings of the ICML 2017.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca34787-cc08-4add-98b3-35bb42876e5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.3 Seq2Seq 与 Attention 机制的兴起"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e6f5c9-c0fe-4671-a82c-235782ad3c73",
   "metadata": {},
   "source": [
    "### 1.3.1 Seq2Seq 框架的提出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645a4e01-9d30-400d-8cbb-cebfca98453e",
   "metadata": {},
   "source": [
    "在处理 **机器翻译** 等序列到序列（Seq2Seq）任务时，传统的神经网络往往无法很好地处理输入和输出之间复杂的顺序关系。为了有效地处理这些任务，研究者们提出了 **Seq2Seq** 框架，也被称为 **序列到序列模型**。\n",
    "\n",
    "Seq2Seq模型的核心思想是通过一个 **编码器**（Encoder） 将**输入序列压缩成一个固定长度的向量**，然后通过一个 **解码器**（Decoder） 将这个向量**映射回目标序列**。这种 **Encoder-Decoder** 架构在机器翻译中应用广泛，因为它能够自动将源语言的句子（如英文）转换为目标语言的句子（如法文）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4512557b-5f6e-4408-bb88-0ef81b70bdaf",
   "metadata": {},
   "source": [
    "### 1.3.2 Encoder-Decoder 架构在机器翻译中的应用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5814fa-cc0b-429e-b73f-1ba4af97e8f4",
   "metadata": {},
   "source": [
    "在传统的Seq2Seq架构中，**编码器** 通过逐个读取源语言的每个单词，并将其转化为一个隐藏状态向量（通常是RNN或LSTM单元）。**解码器** 则接收这个向量并逐步生成目标语言的单词。**编码器和解码器之间通过一个固定长度的上下文向量进行信息传递**，这个向量代表了整个输入序列的信息。\n",
    "\n",
    "这种结构解决了传统神经网络处理序列数据时 **信息传递的困难**，使得机器能够将长序列输入映射为相对简洁的表示。然而，这种固定长度的上下文向量也存在一个问题，即它**不能有效捕捉长序列中的所有信息**，尤其是当源序列较长时，**固定长度的向量可能无法保留足够的上下文信息**，导致 **信息丢失**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6fda45-a053-4b59-b513-fa25077e3bf7",
   "metadata": {},
   "source": [
    "### 1.3.3 Attention 机制的引入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8433e7-6f38-416c-b9e6-45ed13b905d7",
   "metadata": {},
   "source": [
    "为了克服传统Seq2Seq框架的局限性，**Attention机制** 被引入到了机器翻译和其他序列建模任务中。Attention机制的核心思想是，在每个解码步骤中，解码器能够**选择性地关注源序列中的不同部分**，而**不仅仅依赖于一个固定长度的上下文向量**。通过这种方式，Attention机制使得模型能够更灵活地捕捉 **长距离依赖关系**。\n",
    "\n",
    "**(1) 解决长距离依赖问题**\n",
    "\n",
    "在传统的Seq2Seq框架中，编码器生成的固定长度上下文向量被解码器用作输入。然而，当源序列较长时，这个固定长度的向量无法有效地表示所有信息。Attention机制通过让解码器在每个步骤中关注源序列的不同部分，解决了这一问题。\n",
    "\n",
    "具体来说，Attention机制计算出一个 **权重分布**，该分布决定了每个源词对当前解码步骤的影响程度。这样，解码器可以根据当前生成的目标词与源序列的关系，动态地调整注意力焦点，从而更好地捕捉到源序列中的重要信息，尤其是长距离依赖关系。\n",
    "\n",
    "**(2) 提升翻译与生成效果**\n",
    "\n",
    "Attention机制的引入大大提升了Seq2Seq模型在翻译和文本生成中的效果。通过让模型在解码时自适应地选择关注的输入信息，翻译质量得到了显著提高，尤其是在翻译较长的句子时。Attention机制使得生成的目标文本更加准确和自然。\n",
    "\n",
    "例如，在机器翻译任务中，Attention机制不仅可以增强对单词之间上下文的理解，还能使得生成的翻译句子更加流畅和有条理，避免了传统Seq2Seq模型中的语法不连贯或信息丢失的问题。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905f1d0e-a4d4-46ef-9edd-80ddad0fa024",
   "metadata": {},
   "source": [
    "### 1.3.4 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa3dfd-f83e-4fc4-8f36-e61184cd62c9",
   "metadata": {},
   "source": [
    "- Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to Sequence Learning with Neural Networks. In Advances in Neural Information Processing Systems (NIPS 2014).\n",
    "\n",
    "- Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural Machine Translation by Jointly Learning to Align and Translate. In Proceedings of the ICLR 2015.\n",
    "\n",
    "- Luong, M. T., Pham, H., & Manning, C. D. (2015). Effective Approaches to Attention-based Neural Machine Translation. In Proceedings of the EMNLP 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a1cca-570f-446a-8f6a-42e9f2ba0449",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.4 Transformer 的提出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b04a4c8-96d8-4f6f-84cb-875d77767416",
   "metadata": {},
   "source": [
    "### 1.4.1 核心创新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85e00b3-7344-4916-ab75-a0c870df4ef3",
   "metadata": {},
   "source": [
    "**(1) 完全基于 Self-Attention 的架构**\n",
    "\n",
    "在传统的神经网络模型中，如 **RNN** 和 **CNN**，信息的传递通常依赖于序列中前一个元素的处理结果（如循环或卷积操作）。然而，随着序列长度的增加，这些方法面临着严重的计算瓶颈，尤其是无法有效处理长距离依赖。为了解决这一问题，Transformer 架构提出了 Self-Attention（自注意力）机制，这成为其最大的创新。\n",
    "\n",
    "Self-Attention 机制允许模型在处理每个单词时，能够根据当前词与其它词的关系**动态调整其关注重点**。与传统方法不同，Self-Attention 不依赖固定的前一个信息，而是**计算所有输入词之间的相对关系**，从而 **并行化处理** 序列数据，极大提升了计算效率。\n",
    "\n",
    "具体来说，Self-Attention 通过计算查询（Query）、键（Key）、值（Value）三者的点积关系来衡量**单词之间的相似性**，得到一个**注意力分数**，再根据这个分数**加权其他单词的影响**。这使得每个单词都可以根据上下文信息动态调整其表示。\n",
    "\n",
    "**(2) 去掉循环与卷积，提升并行性与效率**\n",
    "\n",
    "传统的RNN和LSTM通过逐步传递信息来处理序列数据，这种方式固有的顺序结构使得计算过程难以并行化。而卷积神经网络虽然在局部特征提取方面表现良好，但其处理长距离依赖的能力有限。\n",
    "\n",
    "**(3) Multi-Head Attention、位置编码、残差连接等关键组件**\n",
    "\n",
    "- **Multi-Head Attention**\n",
    "在传统的 Self-Attention 中，每个单词的表示都是通过对其他单词进行加权平均得到的，然而不同的上下文信息可能需要关注不同的子空间。Multi-Head Attention 通过并行地应用多个不同的 attention 操作，使得模型能够从多个角度捕捉输入信息的不同特征，从而增强模型的表示能力。\n",
    "\n",
    "- **位置编码（Positional Encoding）**\n",
    "由于 Transformer 摒弃了传统的循环结构，它无法像 RNN 那样通过时序来理解单词之间的顺序。为了解决这个问题，Transformer 引入了 位置编码，通过向每个输入单词添加一个独特的位置信息，使得模型能够理解序列中的词汇顺序。\n",
    "\n",
    "- **残差连接（Residual Connection）**\n",
    "Transformer 中的每个子层（如 Self-Attention 层和前馈神经网络层）都通过残差连接进行连接，这种设计有助于缓解深层网络中的梯度消失问题，并加速模型的训练过程。\n",
    "\n",
    "这些创新使得 Transformer 不仅能够处理长序列的依赖关系，还能在训练过程中获得显著的效率提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c77e2e4-fb1c-4a25-b75d-ef367bb0d9c1",
   "metadata": {},
   "source": [
    "### 1.4.2 论文贡献"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1240b221-af13-4ca6-b71e-4f6eafe83b6f",
   "metadata": {},
   "source": [
    "`Transformer` 架构的提出由 Vaswani 等人在 2017 年的论文 “Attention Is All You Need” 中首次公布。该论文的发布标志着自然语言处理（NLP）架构的范式转变，传统的循环神经网络（RNN）和卷积神经网络（CNN）开始逐步被基于 Transformer 的模型所取代。\n",
    "\n",
    "论文的主要贡献包括：\n",
    "\n",
    "- 提出了 Self-Attention 机制，这使得模型能够高效地建模长距离依赖。\n",
    "\n",
    "- 完全并行化的计算结构，避免了 RNN 中无法并行的瓶颈，显著提升了训练速度。\n",
    "\n",
    "- 引入 Multi-Head Attention、**位置编码** 和 **残差连接** 等关键设计，为模型的深度学习能力奠定了基础。\n",
    "\n",
    "Transformer 的提出，不仅解决了传统模型在处理序列数据时的诸多局限性，还为后来的 BERT、GPT 等大型预训练模型提供了理论基础，成为 NLP 领域的重要里程碑。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8a9696-c103-47e2-a7b8-1ccfc7ba4093",
   "metadata": {},
   "source": [
    "### 1.4.3 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b3ac45-956c-4b14-a26e-f358b842fac5",
   "metadata": {},
   "source": [
    "- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention Is All You Need. In Proceedings of NeurIPS 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d68597-d681-4c38-ab0e-da65f4a91b06",
   "metadata": {},
   "source": [
    "## 1.5 残差结构与LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ecd22-b127-4ff3-b2d4-eba6eb72a41d",
   "metadata": {},
   "source": [
    "### 1.5.1 残差连接：为何使用残差连接？它对模型训练有什么帮助？\n",
    "\n",
    "**(1) 残差连接的提出**\n",
    "\n",
    "在深度神经网络的训练中，随着网络层数的增加，模型容易遇到 **梯度消失**（Vanishing Gradient） 或 **梯度爆炸**（Exploding Gradient） 的问题，导致网络无法有效训练。为了应对这一问题，**残差连接**（Residual Connection） 被引入到神经网络中。\n",
    "\n",
    "残差连接的核心思想是将**前一层的输出直接加到当前层的输入**中，形成一个 **跳跃连接**，这样模型在传递梯度时可以避免梯度在每一层之间的消失或爆炸。这种设计允许信息在网络中“跳跃”通过多个层，而不必在每一层之间都被转换和损失，从而使得网络能够更容易地进行训练。\n",
    "\n",
    "**(2) 为何使用残差连接？**\n",
    "\n",
    "残差连接的引入有以下几方面的原因：\n",
    "\n",
    "1. **缓解梯度消失和梯度爆炸问题**\n",
    "在深层网络中，梯度可能在反向传播过程中逐渐减小（梯度消失），或者变得非常大（梯度爆炸）。残差连接通过将输入直接传递到输出层，确保了梯度能够直接通过每一层反向传播，从而减少了这些问题。\n",
    "\n",
    "2. **加速网络训练**\n",
    "通过让每一层的输出不完全依赖于前一层的处理结果，残差连接使得网络更容易训练，尤其是在训练深层网络时。每一层的网络可以学习到从输入到输出的“残差”部分，而不是学习复杂的直接映射，这大大加快了收敛速度。\n",
    "\n",
    "3. **使深层网络更易优化**\n",
    "残差连接可以看作是网络学习到的一种恒等映射，允许网络**仅学习如何调整输入的特征**，而不是学习**复杂的转换**。这种设计使得网络能够从浅层到深层逐渐积累信息，并且不会受到梯度消失的限制。\n",
    "\n",
    "**(3) 残差连接在Transformer中的作用**\n",
    "\n",
    "在 **Transformer** 中，每个子层（如 Self-Attention 层、前馈神经网络层）后都加入了残差连接。每个子层的输出会与输入加和，然后通过**层归一化**（LayerNorm）处理。残差连接使得 Transformer 可以**在多层网络中更好地传递信息**，避免了因网络层数加深导致的信息损失，使得模型更加稳定和易于训练。\n",
    "\n",
    "### 1.5.2 LayerNorm vs. BatchNorm：比较两者的优缺点，Transformer为何选择LayerNorm？\n",
    "\n",
    "**(1) BatchNorm（批量归一化）**\n",
    "\n",
    "**Batch Normalization**（批量归一化） 是在训练深层网络时常用的一种技术，它的作用是对每一层的输入进行标准化处理，使得每一层的输入具有**零均值**和**单位方差**，从而加速训练过程并改善模型的性能。BatchNorm通过以下方式工作：\n",
    "\n",
    "- 在训练时，计算每一层的输入的均值和方差。\n",
    "\n",
    "- 对输入进行标准化，并使用 **可学习的缩放系数** 和 **偏置系数** 来调整标准化后的输出。\n",
    "\n",
    "这种方法能够**有效地缓解梯度消失问题**，加速训练，并增强模型的泛化能力。\n",
    "\n",
    "**(2) LayerNorm（层归一化）**\n",
    "\n",
    "**Layer Normalization**（层归一化） 与 BatchNorm 相似，但它有一些关键的不同之处。与 BatchNorm 在 每个批次的样本 上进行归一化不同，LayerNorm 是对 每个样本的所有特征 进行归一化。换句话说，LayerNorm是对输入的每个单独样本的每个特征维度进行标准化，而不是跨批次计算均值和方差。\n",
    "\n",
    "### 1.5.3 总结\n",
    "\n",
    "- **残差连接** 通过跳跃连接的方式帮助缓解深层网络中的**梯度消失问题**，使得模型更易训练，特别是在深度网络（如 Transformer）中。\n",
    "\n",
    "- **LayerNorm** 在处理序列数据时相比 **BatchNorm** 具有更大的**灵活性**，避免了因批次大小和序列顺序问题带来的负面影响，因此成为 **Transformer** 架构中的核心组件。\n",
    "\n",
    "### 1.5.4 参考文献\n",
    "\n",
    "- He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep Residual Learning for Image Recognition. In Proceedings of CVPR 2016.\n",
    "\n",
    "- Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer Normalization. In Proceedings of NeurIPS 2016.\n",
    "\n",
    "- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention Is All You Need. In Proceedings of NeurIPS 2017.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
